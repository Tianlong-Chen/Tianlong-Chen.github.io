{"meta":{"title":"Tianlong Chen (陈天龙)","subtitle":"What does not kill you makes you stronger","description":"Make the change that you want to see in the world","author":"Tianlong Chen (陈天龙)","url":"https://Tianlong-Chen.github.io","root":"/"},"pages":[{"title":"Hello World!","date":"2020-07-23T00:33:50.656Z","updated":"2020-07-23T00:33:50.650Z","comments":true,"path":"about/index.html","permalink":"https://tianlong-chen.github.io/about/index.html","excerpt":"","text":"I am currently a second-year Ph.D. student of Computer Science at VITA, Texas A&amp;M University, adverised by Dr. Zhangyang (Atlas) Wang. My research interests include AutoML, Adversarial Robustness, Self-Supervision and Graph Neural Networks. [Resume] [Google Scholar] [Publication] Education [Aug. 2018 - Present] Ph.D. in Computer Science, Texas A&amp;M University [Aug. 2013 - Jun. 2017] B.S.c. in Applied Mathematics, School of the Gifted Young, University of Science and Technology of China [Aug. 2013 - Jun. 2017] B.Eng. (Minor) in Computer Science, School of the Gifted Young, University of Science and Technology of China Publication[*equal contribution] 2020 &nbsp;&nbsp;[ECCV’20] HALO: Hardware-Aware Learning to Optimize&nbsp;&nbsp;T. Chen*, C. Li*, H. You, Z. Wang, and Y. Lin.&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[ICML’20] When Does Self-Supervision Help GCN?&nbsp;&nbsp;T. Chen*, Y. You*, Z. Wang, and Y. Shen&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[ICML’20] Self-PU: Self Boosted and Calibrated PU Training&nbsp;&nbsp;X. Chen*, W. Chen*, T. Chen, Y. Yuan, C. Gong, K. Chen, and Z. Wang&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[CVPR’20] Adv. Robustness: Self-Supervised Pre-Training&nbsp;&nbsp;T. Chen, S. Liu, S. Chang, Y. Cheng, L. Amini, and Z. Wang&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[CVPR’20] L^2-GCN: Layer-Wise Efficient Training of GCN&nbsp;&nbsp;T. Chen*, Y. You*, Z. Wang, and Y. Shen&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[ICLR’20] Triple Wins: Boosting Accuracy, Robustness and Efficiency&nbsp;&nbsp;T. Chen*, T. Hu*, H. Wang, and Z. Wang&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[ICLR’20] I am Going MAD: Maximum Discrepancy Competition&nbsp;&nbsp;H. Wang, T. Chen, Z. Wang, and K. Ma&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[WACV’20] Domain-Invariant Learning for Generalizable Re-ID&nbsp;&nbsp;Y. Yuan, W. Chen, T. Chen, Y. Yang, Z. Ren, Z. Wang and G. Hua&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[LREC’20] Eligibility Criteria-to-SQL Semantic Parsing&nbsp;&nbsp;X. Yu, T. Chen, Z. Yu, H. Li, Y. Yang, X. Jiang and A. Jiang&nbsp;&nbsp;[Paper] [Abstract] &nbsp;&nbsp;[CVPRW’20] Focus Longer to See Better: Recursively Attention&nbsp;&nbsp;P. Shroff, T. Chen, Y. Wei, and Z. Wang&nbsp;&nbsp;[Paper] [Code] [Abstract] 2019 &nbsp;&nbsp;[NeurIPS’19] Learning to Optimize in Swarms&nbsp;&nbsp;Y. Cao, T. Chen, Z. Wang, and S. Yang&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[ICCV’19] ABD-Net: Attentive but Diverse Person Re-Identification&nbsp;&nbsp;T. Chen, S. Ding*, J. Xie*, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and Z. Wang&nbsp;&nbsp;[Paper] [Code] [Abstract] &nbsp;&nbsp;[ICCVW’19] Cross-Model Person Search: Text-Image Matching&nbsp;&nbsp;T. Chen*, X. Yu*, Y. Yang, M. Mugo, and Z. Wang&nbsp;&nbsp;[Paper] [Abstract] Experience Facebook Research, Research Intern [Sep. 2020 - Dec. 2020] Mentor: Dr. Xing Wang Microsoft Research Redmond, Research Intern [May. 2020 - Aug. 2020] Mentor: Dr. Yu Cheng, Dr. Zhe Gan, Dr. Yu Hu Walmart Technology, Research Intern [May. 2019 - Aug. 2019] Mentor: Dr. Yang Yang University of Science and Technology of China, Research Assistant [Jun. 2017 - Jun. 2018] Mentor: Dr. Zhouwang Yang Harvard University, Research Intern [Jul. 2016 - Jan. 2017] Mentor: Dr. Gil Alterovitz Research Award 3rd Place, ICCV 2019 WIDER Challenge Track 4, Oct. 2019 First Place, Walmart Person Re-Identification (Re-ID) Competition, May. 2018 NIPS 2019 Student Travel Award, Dec. 2019 More About Me I am a huge fan of Pokemon. Playing Pokemon Go is one of daily activities. I also enjoy Hip-Hop and Country music. Air (艾热) is one of my favorite Chinese Hip-Hop stars."},{"title":"categories","date":"2018-08-01T05:00:00.000Z","updated":"2020-07-22T02:09:00.133Z","comments":true,"path":"categories/index.html","permalink":"https://tianlong-chen.github.io/categories/index.html","excerpt":"","text":""},{"title":"Topics","date":"2018-08-01T05:00:00.000Z","updated":"2020-07-22T07:23:22.839Z","comments":true,"path":"tags/index.html","permalink":"https://tianlong-chen.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"(ECCV 2020) HALO, Hardware-Aware Learning to Optimize","slug":"ECCV20-HALO","date":"2020-08-23T05:00:00.000Z","updated":"2020-07-22T04:15:04.192Z","comments":true,"path":"ECCV20-HALO/","link":"","permalink":"https://tianlong-chen.github.io/ECCV20-HALO/","excerpt":"","text":"HALO: Hardware-Aware Learning to Optimize[Paper] [Code] AbstractThere has been an explosive demand for bringing machine learning (ML) powered intelligence into numerous Internet-of-Things (IoT) devices. However, the effectiveness of such intelligent functionality requires in-situ continuous model adaptation for adapting to new dataand environments, while the on-device computing and energy resources are usually extremely constrained. Neither traditional hand-crafted (e.g., SGD, Adagrad, and Adam) nor existing meta optimizers are specifically designed to meet those challenges, as the former requires tedious hyper-parameter tuning while the latter are often costly due to themeta algorithms’ own overhead. To this end, we propose hardware-aware learning to optimize (HALO), a practical meta optimizer dedicated to resource-efficient on-device adaptation. Our HALO optimizer features the following highlights: (1) faster adaptation speed (i.e., taking fewer data oriterations to reach a specified accuracy) by introducing a new regularizer to promote empirical generalization; and (2) lower per-iteration complexity, thanks to a stochastic structural sparsity regularizer being enforced. Furthermore, the optimizer itself is designed as a very light-weight RNN and thus incurs negligible overhead. Ablation studies and experiments onfive datasets, six optimizees, and two state-of-the-art (SOTA) edge AIdevices validate that, while always achieving a better accuracy (↑0.46% -↑20.28%), HALO can greatly trim down the energy cost (up to↓60%) inadaptation, quantified using an IoT device or SOTA simulator. Codesand pre-trained models are at https://github.com/RICE-EIC/HALO .","categories":[{"name":"ECCV'20","slug":"ECCV-20","permalink":"https://tianlong-chen.github.io/categories/ECCV-20/"}],"tags":[{"name":"Adaptation","slug":"Adaptation","permalink":"https://tianlong-chen.github.io/tags/Adaptation/"},{"name":"Learning to Optimize","slug":"Learning-to-Optimize","permalink":"https://tianlong-chen.github.io/tags/Learning-to-Optimize/"},{"name":"Efficient Training","slug":"Efficient-Training","permalink":"https://tianlong-chen.github.io/tags/Efficient-Training/"},{"name":"Efficient Inference","slug":"Efficient-Inference","permalink":"https://tianlong-chen.github.io/tags/Efficient-Inference/"}]},{"title":"(ICML 2020) When Does Self-Supervision Help Graph Convolutional Networks?","slug":"ICML20-GCN","date":"2020-07-12T05:00:00.000Z","updated":"2020-07-22T05:49:26.990Z","comments":true,"path":"ICML20-GCN/","link":"","permalink":"https://tianlong-chen.github.io/ICML20-GCN/","excerpt":"","text":"When Does Self-Supervision Help Graph Convolutional Networks?[Paper] [Code] AbstractSelf-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of images. Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into GCNs. We first elaborate three mechanisms to incorporate self-supervision into GCNs, analyze the limitations of pretraining &amp; finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we propose to investigate threenovel self-supervised learning tasks for GCNs with theoretical rationales and numerical comparisons. Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness. Our codes are available at https://github.com/Shen-Lab/SS-GCNs .","categories":[{"name":"ICML'20","slug":"ICML-20","permalink":"https://tianlong-chen.github.io/categories/ICML-20/"}],"tags":[{"name":"Graph Neural Networks","slug":"Graph-Neural-Networks","permalink":"https://tianlong-chen.github.io/tags/Graph-Neural-Networks/"},{"name":"Adversarial Robustness","slug":"Adversarial-Robustness","permalink":"https://tianlong-chen.github.io/tags/Adversarial-Robustness/"},{"name":"Self-Supervision","slug":"Self-Supervision","permalink":"https://tianlong-chen.github.io/tags/Self-Supervision/"}]},{"title":"(ICML 2020) Self-PU, Self Boosted and Calibrated Positive-Unlabeled Training","slug":"ICML20-Self","date":"2020-07-11T05:00:00.000Z","updated":"2020-07-22T03:52:43.880Z","comments":true,"path":"ICML20-Self/","link":"","permalink":"https://tianlong-chen.github.io/ICML20-Self/","excerpt":"","text":"Self-PU: Self Boosted and Calibrated Positive-Unlabeled Training[Paper] [Code] AbstractMany real-world applications have to tackle the Positive-Unlabeled (PU) learning problem,i.e.,learning binary classifiers from a large amountof unlabeled data and a few labeled positive examples. While current state-of-the-art methodsemploy importance reweighting to design various risk estimators, they ignored the learning capability of the model itself, which could haveprovided reliable supervision. This motivatesus to propose a novel Self-PU learning framework, which seamlessly integrates PU learningand self-training. Self-PU highlights three “self”-oriented building blocks: a self-paced training algorithm that adaptively discovers and augments confident positive/negative examples as the training proceeds; a self-calibrated instance-aware loss; and a self-distillation scheme that introduces teacher-students learning as an effectiveregularization for PU learning. We demonstratethe state-of-the-art performance of Self-PU oncommon PU learning benchmarks (MNIST and CIFAR-10), which compare favorably against thelatest competitors. Moreover, we study a real-world application of PU learning,i.e., classifying brain images of Alzheimer’s Disease. Self-PU obtains significantly improved results on the renowned Alzheimer’s Disease Neuroimaging Initiative (ADNI) database over existing methods.","categories":[{"name":"ICML'20","slug":"ICML-20","permalink":"https://tianlong-chen.github.io/categories/ICML-20/"}],"tags":[{"name":"Learning to Optimize","slug":"Learning-to-Optimize","permalink":"https://tianlong-chen.github.io/tags/Learning-to-Optimize/"},{"name":"PU Learning","slug":"PU-Learning","permalink":"https://tianlong-chen.github.io/tags/PU-Learning/"},{"name":"Self Training","slug":"Self-Training","permalink":"https://tianlong-chen.github.io/tags/Self-Training/"}]},{"title":"(CVPR 2020) Adversarial Robustness, From Self-Supervised Pre-Training to Fine-Tuning","slug":"CVPR20-Selfie","date":"2020-06-18T05:00:00.000Z","updated":"2020-07-22T04:15:58.249Z","comments":true,"path":"CVPR20-Selfie/","link":"","permalink":"https://tianlong-chen.github.io/CVPR20-Selfie/","excerpt":"","text":"Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning[Paper] [Code] AbstractPretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We introduce adversarial training into self- supervision, to provide general-purpose robust pretrained models for the first time. We find these robust pretrained models can benefit the subsequent fine-tuning in two ways: i) boosting final model robustness; ii) saving the computation cost, if proceeding towards adversarial fine-tuning. We conduct extensive experiments to demonstrate that the proposed framework achieves large performance margins (e.g., 3.83% on robust accuracy and 1.3% on standard accuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end adversarial training baseline. Moreover, we find that different self-supervised pretrained models have diverse adversarial vulnerability. It inspires us to ensemble several pretraining tasks, which boosts robustness more. Our ensemble strategy contributes to a further improvement of 3.59% on robust accuracy, while maintaining a slightly higher standard accuracy on CIFAR-10. Our codes are available at https://github.com/TAMU-VITA/Adv-SS-Pretraining .","categories":[{"name":"CVPR'20","slug":"CVPR-20","permalink":"https://tianlong-chen.github.io/categories/CVPR-20/"}],"tags":[{"name":"Adversarial Robustness","slug":"Adversarial-Robustness","permalink":"https://tianlong-chen.github.io/tags/Adversarial-Robustness/"},{"name":"Self-Supervision","slug":"Self-Supervision","permalink":"https://tianlong-chen.github.io/tags/Self-Supervision/"},{"name":"Pre-Training","slug":"Pre-Training","permalink":"https://tianlong-chen.github.io/tags/Pre-Training/"},{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"https://tianlong-chen.github.io/tags/Transfer-Learning/"}]},{"title":"(CVPR 2020) L^2-GCN, Layer-Wise and Learned Efficient Training of Graph Convolutional Networks","slug":"CVPR20-GCN","date":"2020-06-17T05:00:00.000Z","updated":"2020-07-22T03:23:57.130Z","comments":true,"path":"CVPR20-GCN/","link":"","permalink":"https://tianlong-chen.github.io/CVPR20-GCN/","excerpt":"","text":"L$^2$-GCN: Layer-Wise and Learned Efficient Training of Graph Convolutional Networks[Paper] [Code] AbstractGraph convolution networks (GCN) are increasingly popular in many applications, yet remain notoriously hard to train over large graph datasets. They need to compute node representations recursively from their neighbors. Current GCN training algorithms suffer from either high computational costs that grow exponentially with the number of layers, or high memory usage for loading the entire graph and node embeddings. In this paper, we propose a novel efficient layer-wise training framework for GCN (L-GCN), that disentangles feature aggregation and feature transformation during training, hence greatly reducing time and memory complexities. We present theoretical analysis for L-GCN under the graph isomorphism framework, that L-GCN leads to as powerful GCNs as the more costly conventional training algorithm does, under mild conditions. We further propose L$^2$-GCN, which learns a controller for each layer that can automatically adjust the training epochs per layer in L-GCN. Experiments show that L-GCN is faster than state-of-the-arts by at least an order of magnitude, with a consistent of memory usage not dependent on dataset size, while maintaining comparable prediction performance. With the learned controller, L$^2$-GCN can further cut the training time in half. Our codes are available at https://github.com/Shen-Lab/L2-GCN .","categories":[{"name":"CVPR'20","slug":"CVPR-20","permalink":"https://tianlong-chen.github.io/categories/CVPR-20/"}],"tags":[{"name":"Learning to Optimize","slug":"Learning-to-Optimize","permalink":"https://tianlong-chen.github.io/tags/Learning-to-Optimize/"},{"name":"Efficient Training","slug":"Efficient-Training","permalink":"https://tianlong-chen.github.io/tags/Efficient-Training/"},{"name":"Graph Neural Networks","slug":"Graph-Neural-Networks","permalink":"https://tianlong-chen.github.io/tags/Graph-Neural-Networks/"}]},{"title":"(CVPRW 2020) Focus Longer to See Better, Recursively Refined Attention for Fine-Grained Image Classification","slug":"CVPRW20","date":"2020-06-16T05:00:00.000Z","updated":"2020-07-22T06:35:33.817Z","comments":true,"path":"CVPRW20/","link":"","permalink":"https://tianlong-chen.github.io/CVPRW20/","excerpt":"","text":"Focus Longer to See Better: Recursively Refined Attention for Fine-Grained Image Classification[Paper] [[Code](https://github.com/ TAMU-VITA/Focus-Longer-to-See-Better)] AbstractDeep Neural Network has shown great strides in the coarse-grained image classification task. It was in part due to its strong ability to extract discriminative feature representations from the images. However, the marginal visual difference between different classes in fine-grained images makes this very task harder. In this paper, we tried to focus on these marginal differences to extract more representative features. Similar to human vision, our network repetitively focuses on parts of images to spot small discriminative parts among the classes. Moreover, we show through interpretability techniques how our network focus changes from coarse to fine details. Through our experiments, we also show that a simple attention model can aggregate (weighted) these finer details to focus on the most dominant discriminative part of the image. Our network uses only image-level labels and does not need bounding box/part annotation information. Further, the simplicity of our network makes it an easy plug-n-play module. Apart from providing interpretability, our network boosts the performance (up to 2%) when compared to its baseline counterparts. Our codebase is available at https://github.com/TAMU-VITA/Focus-Longer-to-See-Better .","categories":[{"name":"CVPRW'20","slug":"CVPRW-20","permalink":"https://tianlong-chen.github.io/categories/CVPRW-20/"}],"tags":[{"name":"Attention","slug":"Attention","permalink":"https://tianlong-chen.github.io/tags/Attention/"},{"name":"Fine-Grained Classification","slug":"Fine-Grained-Classification","permalink":"https://tianlong-chen.github.io/tags/Fine-Grained-Classification/"}]},{"title":"(LREC 2020) Dataset and Enhanced Model for Eligibility Criteria-to-SQL Semantic Parsing","slug":"LREC20","date":"2020-05-11T05:00:00.000Z","updated":"2020-07-22T02:41:25.345Z","comments":true,"path":"LREC20/","link":"","permalink":"https://tianlong-chen.github.io/LREC20/","excerpt":"","text":"Dataset and Enhanced Model for Eligibility Criteria-to-SQL Semantic Parsing[Paper] AbstractClinical trials often require that patients meet eligibility criteria (e.g., have specific conditions) to ensure the safety and the effectiveness of studies. However, retrieving eligible patients for a trial from the electronic health record (EHR) database remains a challenging task for clinicians since it requires not only medical knowledge about eligibility criteria, but also an adequate understanding of structured query language (SQL). In this paper, we introduce a new dataset that includes the first-of-its-kind eligibility-criteria corpus and the corresponding queries for criteria-to-sql (Criteria2SQL), a task translating the eligibility criteria to executable SQL queries. Compared to existing datasets, the queries in the dataset here are derived from the eligibility criteria of clinical trials and include Order-sensitive, Counting-based, and Boolean-type cases which are not seen before. In addition to the dataset, we propose a novel neural semantic parser as a strong baseline model. Extensive experiments show that the proposed parser outperforms existing state-of-the-art general-purpose text-to-sql models while highlighting the challenges presented by the new dataset. The uniqueness and the diversity of the dataset leave a lot of research opportunities for future improvement.","categories":[{"name":"LREC'20","slug":"LREC-20","permalink":"https://tianlong-chen.github.io/categories/LREC-20/"}],"tags":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"https://tianlong-chen.github.io/tags/Natural-Language-Processing/"},{"name":"BERT","slug":"BERT","permalink":"https://tianlong-chen.github.io/tags/BERT/"}]},{"title":"(ICLR 2020) Triple Wins, Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference","slug":"ICLR20-Triple","date":"2020-05-05T05:00:00.000Z","updated":"2020-07-22T04:08:50.388Z","comments":true,"path":"ICLR20-Triple/","link":"","permalink":"https://tianlong-chen.github.io/ICLR20-Triple/","excerpt":"","text":"Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference[Paper] [Code] AbstractDeep networks were recently suggested to face the odds between accuracy (on clean natural images) and robustness (on adversarially perturbed images) (Tsipras et al., 2019). Such a dilemma is shown to be rooted in the inherently higher sample complexity (Schmidt et al., 2018) and/or model capacity (Nakkiran, 2019), for learning a high-accuracy and robust classifier. In view of that, give a classification task, growing the model capacity appears to help draw a win-win between accuracy and robustness, yet at the expense of model size and latency, therefore posing challenges for resource-constrained applications. Is it possible to co-design model accuracy, robustness and efficiency to achieve their triple wins? This paper studies multi-exit networks associated with input-adaptive efficient inference, showing their strong promise in achieving a “sweet point” in co- optimizing model accuracy, robustness and efficiency. Our proposed solution, dubbed Robust Dynamic Inference Networks (RDI-Nets), allows for each input (either clean or adversarial) to adaptively choose one of the multiple output layers (early branches or the final one) to output its prediction. That multi-loss adaptivity adds new variations and flexibility to adversarial attacks and defenses, on which we present a systematical investigation. We show experimentally that by equipping existing backbones with such robust adaptive inference, the resulting RDI-Nets can achieve better accuracy and robustness, yet with over 30% computational savings, compared to the defended original models.","categories":[{"name":"ICLR'20","slug":"ICLR-20","permalink":"https://tianlong-chen.github.io/categories/ICLR-20/"}],"tags":[{"name":"Efficient Training","slug":"Efficient-Training","permalink":"https://tianlong-chen.github.io/tags/Efficient-Training/"},{"name":"Efficient Inference","slug":"Efficient-Inference","permalink":"https://tianlong-chen.github.io/tags/Efficient-Inference/"},{"name":"Adversarial Robustness","slug":"Adversarial-Robustness","permalink":"https://tianlong-chen.github.io/tags/Adversarial-Robustness/"},{"name":"Adaptive Inference","slug":"Adaptive-Inference","permalink":"https://tianlong-chen.github.io/tags/Adaptive-Inference/"}]},{"title":"(ICLR 2020) I am Going MAD, Maximum Discrepancy Competition for Comparing Classifiers Adaptively","slug":"ICLR20-MAD","date":"2020-05-04T05:00:00.000Z","updated":"2020-07-22T06:35:51.621Z","comments":true,"path":"ICLR20-MAD/","link":"","permalink":"https://tianlong-chen.github.io/ICLR20-MAD/","excerpt":"","text":"I am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively[Paper] [Code] AbstractThe learning of hierarchical representations for image classification has experienced an impressive series of successes due in part to the availability of large-scale labeled data for training. On the other hand, the trained classifiers have traditionally been evaluated on small and fixed sets of test images, which are deemed to be extremely sparsely distributed in the space of all natural images. It is thus questionable whether recent performance improvements on the excessively re-used test sets generalize to real-world natural images with much richer content variations. Inspired by efficient stimulus selection for testing perceptual models in psychophysical and physiological studies, we present an alternative framework for comparing image classifiers, which we name the MAximum Discrepancy (MAD) competition. Rather than comparing image classifiers using fixed test images, we adaptively sample a small test set from an arbitrarily large corpus of unlabeled images so as to maximize the discrepancies between the classifiers, measured by the distance over WordNet hierarchy. Human labeling on the resulting model-dependent image sets reveals the relative performance of the competing classifiers, and provides useful insights on potential ways to improve them. We report the MAD competition results of eleven ImageNet classifiers while noting that the framework is readily extensible and cost-effective to add future classifiers into the competition. Codes can be found at https://github.com/TAMU-VITA/MAD .","categories":[{"name":"ICLR'20","slug":"ICLR-20","permalink":"https://tianlong-chen.github.io/categories/ICLR-20/"}],"tags":[{"name":"Model Comparison","slug":"Model-Comparison","permalink":"https://tianlong-chen.github.io/tags/Model-Comparison/"}]},{"title":"(WACV 2020) Calibrated Domain-Invariant Learning for Highly Generalizable Large Scale Re-Identification","slug":"WACV20","date":"2020-03-01T06:00:00.000Z","updated":"2020-07-22T07:38:53.093Z","comments":true,"path":"WACV20/","link":"","permalink":"https://tianlong-chen.github.io/WACV20/","excerpt":"","text":"Calibrated Domain-Invariant Learning for Highly Generalizable Large Scale Re-Identification[Paper] [Code] AbstractMany real-world applications, such as city scale traffic monitoring and control, requires large scale re-identification. However, previous ReID methods often failed to address two limitations in existing ReID benchmarks, i.e., low spatiotemporal coverage and sample imbalance. Notwithstanding their demonstrated success in every single benchmark, they have difficulties in generalizing to unseen environments. As a re∂sult, these methods are less applicable in a large scale setting due to poor generalization. In seek for a highly generaliz- able large-scale ReID method, we present an adversarial domain-invariant feature learning framework (ADIN) that explicitly learns to separate identity-related features from challenging variations, where for the first time “free” anno- tations in ReID data such as video timestamp and camera index are utilized. Furthermore, we find that the imbalance of nuisance classes jeopardizes the adversarial training, and for mitigation we propose a calibrated adversarial loss that is attentive to nuisance distribution. Experiments on existing large-scale person/vehicle ReID datasets demonstrate that ADIN learns more robust and generalizable representations, as evidenced by its outstanding direct transfer performance across datasets, which is a criterion that can better measure the generalizability of large scale Re-ID methods.","categories":[{"name":"WACV'20","slug":"WACV-20","permalink":"https://tianlong-chen.github.io/categories/WACV-20/"}],"tags":[{"name":"Person Re-Identification","slug":"Person-Re-Identification","permalink":"https://tianlong-chen.github.io/tags/Person-Re-Identification/"},{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"https://tianlong-chen.github.io/tags/Transfer-Learning/"},{"name":"Adversarial Learning","slug":"Adversarial-Learning","permalink":"https://tianlong-chen.github.io/tags/Adversarial-Learning/"}]},{"title":"(NeurIPS 2019) Learning to Optimize in Swarms","slug":"NIPS19","date":"2019-12-06T06:00:00.000Z","updated":"2020-07-22T06:33:33.991Z","comments":true,"path":"NIPS19/","link":"","permalink":"https://tianlong-chen.github.io/NIPS19/","excerpt":"","text":"Learning to Optimize in Swarms[Paper] [Code] AbstractLearning to optimize has emerged as a powerful framework for various optimization and machine learning tasks. Current such “meta-optimizers” often learn in the space of continuous optimization algorithms that are point-based and uncertainty- unaware. To overcome the limitations, we propose a meta-optimizer that learns in the algorithmic space of both point-based and population-based optimization algorithms. The meta-optimizer targets at a meta-loss function consisting of both cumulative regret and entropy. Specifically, we learn and interpret the update formula through a population of LSTMs embedded with sample- and feature-level attentions. Meanwhile, we estimate the posterior directly over the global optimum and use an uncertainty measure to help guide the learning process. Empirical results over non-convex test functions and the protein-docking application demonstrate that this new meta-optimizer outperforms existing competitors. The codes are publicly available at: https://github.com/Shen-Lab/LOIS.","categories":[{"name":"NeurIPS'19","slug":"NeurIPS-19","permalink":"https://tianlong-chen.github.io/categories/NeurIPS-19/"}],"tags":[{"name":"Attention","slug":"Attention","permalink":"https://tianlong-chen.github.io/tags/Attention/"},{"name":"Learning to Optimize","slug":"Learning-to-Optimize","permalink":"https://tianlong-chen.github.io/tags/Learning-to-Optimize/"}]},{"title":"(ICCV 2019) ABD-Net, Attentive but Diverse Person Re-Identification.","slug":"ICCV19-ABD","date":"2019-10-27T05:00:00.000Z","updated":"2020-07-22T06:30:09.827Z","comments":true,"path":"ICCV19-ABD/","link":"","permalink":"https://tianlong-chen.github.io/ICCV19-ABD/","excerpt":"","text":"ABD-Net: Attentive but Diverse Person Re-Identification[Paper] [Code] AbstractAttention mechanisms have been found effective for person re-identification (Re-ID). However, the learned “attentive” features are often not naturally uncorrelated or “diverse”, which compromises the retrieval performance based on the Euclidean distance. We advocate the com- plementary powers of attention and diversity for Re-ID, by proposing an Attentive but Diverse Network (ABD-Net). ABD-Net seamlessly integrates attention modules and diversity regularizations throughout the entire network to learn features that are representative, robust, and more dis- criminative. Specifically, we introduce a pair of comple- mentary attention modules, focusing on channel aggregation and position awareness, respectively. Then, we plug in a novel orthogonality constraint that efficiently enforces di- versity on both hidden activations and weights. Through an extensive set of ablation study, we verify that the attentive and diverse terms each contributes to the perfor- mance boosts of ABD-Net. It consistently outperforms exist- ing state-of-the-art methods on there popular person Re-ID benchmarks.","categories":[{"name":"ICCV'19","slug":"ICCV-19","permalink":"https://tianlong-chen.github.io/categories/ICCV-19/"}],"tags":[{"name":"Attention","slug":"Attention","permalink":"https://tianlong-chen.github.io/tags/Attention/"},{"name":"Person Re-Identification","slug":"Person-Re-Identification","permalink":"https://tianlong-chen.github.io/tags/Person-Re-Identification/"}]},{"title":"(ICCVW 2019) Cross-Model Person Search, A Coarse-to-FineFramework using Bi-directional Text-Image Matching","slug":"ICCVW19","date":"2019-10-26T05:00:00.000Z","updated":"2020-07-22T06:31:57.555Z","comments":true,"path":"ICCVW19/","link":"","permalink":"https://tianlong-chen.github.io/ICCVW19/","excerpt":"","text":"Cross-Model Person Search: A Coarse-to-FineFramework using Bi-directional Text-Image Matching[Paper] AbstractSearching person images from a gallery based on natu- ral language descriptions remains to be a challenging and under-explored cross-modal retrieval problem. To improve the accuracy off an image-based retrieval task, e.g., person re-identification (Person Re-Id), re-ranking is known to be an effective post-processing tool. In this paper, we extend re-ranking from uni-modal retrieval to cross-modal retrieval for the first time, and develop a bi-directional coarse-to-fine framework (BCF) for cross-modal person search. Built on a recent state-of-the-art Person Re- Id model , BCF exploits first text-to-image and then image-to-text relevance, in a two-stage refinement fash- ion. BCF ranks competitively against a strong baseline on the newly-introduced WIDER Person Search dataset , boosting validation set performance by 9.01% (top-1) / 3.87 % (mAP) for val1 and 6.60% (top-1) / 3.49% (mAP) for val2, respectively. With a high score, our solution ranks competitively in the ICCV 2019 WIDER Person Search by Language Challenge.","categories":[{"name":"ICCVW'19","slug":"ICCVW-19","permalink":"https://tianlong-chen.github.io/categories/ICCVW-19/"}],"tags":[{"name":"Person Re-Identification","slug":"Person-Re-Identification","permalink":"https://tianlong-chen.github.io/tags/Person-Re-Identification/"},{"name":"Multi-Modality","slug":"Multi-Modality","permalink":"https://tianlong-chen.github.io/tags/Multi-Modality/"}]}],"categories":[{"name":"ECCV'20","slug":"ECCV-20","permalink":"https://tianlong-chen.github.io/categories/ECCV-20/"},{"name":"ICML'20","slug":"ICML-20","permalink":"https://tianlong-chen.github.io/categories/ICML-20/"},{"name":"CVPR'20","slug":"CVPR-20","permalink":"https://tianlong-chen.github.io/categories/CVPR-20/"},{"name":"CVPRW'20","slug":"CVPRW-20","permalink":"https://tianlong-chen.github.io/categories/CVPRW-20/"},{"name":"LREC'20","slug":"LREC-20","permalink":"https://tianlong-chen.github.io/categories/LREC-20/"},{"name":"ICLR'20","slug":"ICLR-20","permalink":"https://tianlong-chen.github.io/categories/ICLR-20/"},{"name":"WACV'20","slug":"WACV-20","permalink":"https://tianlong-chen.github.io/categories/WACV-20/"},{"name":"NeurIPS'19","slug":"NeurIPS-19","permalink":"https://tianlong-chen.github.io/categories/NeurIPS-19/"},{"name":"ICCV'19","slug":"ICCV-19","permalink":"https://tianlong-chen.github.io/categories/ICCV-19/"},{"name":"ICCVW'19","slug":"ICCVW-19","permalink":"https://tianlong-chen.github.io/categories/ICCVW-19/"}],"tags":[{"name":"Adaptation","slug":"Adaptation","permalink":"https://tianlong-chen.github.io/tags/Adaptation/"},{"name":"Learning to Optimize","slug":"Learning-to-Optimize","permalink":"https://tianlong-chen.github.io/tags/Learning-to-Optimize/"},{"name":"Efficient Training","slug":"Efficient-Training","permalink":"https://tianlong-chen.github.io/tags/Efficient-Training/"},{"name":"Efficient Inference","slug":"Efficient-Inference","permalink":"https://tianlong-chen.github.io/tags/Efficient-Inference/"},{"name":"Graph Neural Networks","slug":"Graph-Neural-Networks","permalink":"https://tianlong-chen.github.io/tags/Graph-Neural-Networks/"},{"name":"Adversarial Robustness","slug":"Adversarial-Robustness","permalink":"https://tianlong-chen.github.io/tags/Adversarial-Robustness/"},{"name":"Self-Supervision","slug":"Self-Supervision","permalink":"https://tianlong-chen.github.io/tags/Self-Supervision/"},{"name":"PU Learning","slug":"PU-Learning","permalink":"https://tianlong-chen.github.io/tags/PU-Learning/"},{"name":"Self Training","slug":"Self-Training","permalink":"https://tianlong-chen.github.io/tags/Self-Training/"},{"name":"Pre-Training","slug":"Pre-Training","permalink":"https://tianlong-chen.github.io/tags/Pre-Training/"},{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"https://tianlong-chen.github.io/tags/Transfer-Learning/"},{"name":"Attention","slug":"Attention","permalink":"https://tianlong-chen.github.io/tags/Attention/"},{"name":"Fine-Grained Classification","slug":"Fine-Grained-Classification","permalink":"https://tianlong-chen.github.io/tags/Fine-Grained-Classification/"},{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"https://tianlong-chen.github.io/tags/Natural-Language-Processing/"},{"name":"BERT","slug":"BERT","permalink":"https://tianlong-chen.github.io/tags/BERT/"},{"name":"Adaptive Inference","slug":"Adaptive-Inference","permalink":"https://tianlong-chen.github.io/tags/Adaptive-Inference/"},{"name":"Model Comparison","slug":"Model-Comparison","permalink":"https://tianlong-chen.github.io/tags/Model-Comparison/"},{"name":"Person Re-Identification","slug":"Person-Re-Identification","permalink":"https://tianlong-chen.github.io/tags/Person-Re-Identification/"},{"name":"Adversarial Learning","slug":"Adversarial-Learning","permalink":"https://tianlong-chen.github.io/tags/Adversarial-Learning/"},{"name":"Multi-Modality","slug":"Multi-Modality","permalink":"https://tianlong-chen.github.io/tags/Multi-Modality/"}]}