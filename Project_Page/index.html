<!doctype html>
<html>

<head>
  <title> Universal Transferrable Tickets from Pre-training</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
    integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">
  <link href="style_extra.css" rel="stylesheet">
</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <p class="lead" style="font-size:300%;">
        Universal Transferrable Tickets from Pre-training
        <address>
                    <nobr>
            <a href="https://tianlong-chen.github.io/about/">Tianlong Chen</a>
            <sup>1</sup>
            ,
          </nobr>
                    <nobr>
            <a href="http://www.jfrankle.com">Jonathan Frankle</a>
            <sup>2</sup>
            ,
          </nobr>
                    <nobr>
            <a href="https://people.csail.mit.edu/chang87/">Shiyu Chang</a>
            <sup>3</sup>
            ,
          </nobr>
                    <nobr>
            <a href="https://lsjxjtu.github.io/index.html">Sijia Liu</a>
            <sup>3, 4</sup>
            ,
          </nobr>
                    <nobr>
            <a href="https://zhangyangbill.github.io/My-webpage/">Yang Zhang</a>
            <sup>3</sup>
            ,
          </nobr>
                    <nobr>
            <a href="https://people.csail.mit.edu/mcarbin/">Michael Carbin</a>
            <sup>2</sup>
            ,
          </nobr>
                    <nobr>
            <a href="https://vita-group.github.io">Zhangyang Wang</a>
            <sup>1</sup>
            
          </nobr>
                    <br>
                    <nobr>University of Texas at Austin,</nobr>
                    <nobr>MIT CSAIL,</nobr>
                    <nobr>MIT-IBM Watson AI Lab, IBM Research,</nobr>
                    <nobr>Michigan State University</nobr>
                  </address>
      </p>
    </div>
  </div> <!-- end nd-pageheader -->

  <div class="container">

    <div class="row">
      <div class="col text-center">
        <p>
                    <a href="https://arxiv.org/pdf/2007.12223.pdf" class="d-inline-block p-3 align-top">
            <img height="60" width="50" src="assets/bert_paper.png" style="border:1px solid"
              data-nothumb><br>
            NeurIPS 2020 Paper
          </a>
                    <a href="https://docs.google.com/presentation/d/1ntlPnIqSfZAtys4hPwl6t8N63oA6_Ym92G24F42n3wg/edit?usp=sharing" class="d-inline-block p-3 align-top">
            <img height="60" width="100" src="assets/bert_poster.png" style="border:1px solid"
              data-nothumb><br>
            NeurIPS 2020 Poster
          </a>
                    <a href="https://github.com/VITA-Group/BERT-Tickets" class="d-inline-block p-3 align-top">
            <img height="60" width="50" src="assets/bert_code.png" style="border:1px solid"
              data-nothumb><br>
            NeurIPS 2020 Code
          </a>
                    <a href="https://arxiv.org/pdf/2012.06908.pdf" class="d-inline-block p-3 align-top">
            <img height="60" width="50" src="assets/CV_paper.png" style="border:1px solid"
              data-nothumb><br>
            CVPR 2021 Paper
          </a>
                    <a href="https://docs.google.com/presentation/d/1lYUo1qpcYRbzm2qYh1mIKjcd8Kg42_oARX1o-44EL4I/edit?usp=sharing" class="d-inline-block p-3 align-top">
            <img height="60" width="100" src="assets/CV_poster.png" style="border:1px solid"
              data-nothumb><br>
            CVPR 2021 Poster
          </a>
                    <a href="https://github.com/VITA-Group/CV_LTH_Pre-training" class="d-inline-block p-3 align-top">
            <img height="60" width="50" src="assets/CV_code.png" style="border:1px solid"
              data-nothumb><br>
            CVPR 2021 Code
          </a>
                </div>
    </div>

    <div class="row">
      <div class="col">

                        <h2>(NeurIPS'20) The Lottery Tickets Hypothesis for Pre-trained BERT Networks</h2>
        
                <p><p align="center">
    <img src="assets/bert_transfer.png" width="80%">
</p>
<p><strong>Abstract:</strong> In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the <em>lottery ticket hypothesis</em> has shown that models for NLP and computer vision contain smaller <em>matching</em> subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40% to 90% sparsity. We find these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer <em>universally</em>; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context.</p></p>
        
                                <h2>(CVPR'21) The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models</h2>
        
                <p><p align="center">
    <img src="assets/T.gif" width="60%">
</p>
<p>Overview of our work paradigm: from pre-trained CV models (both supervised and self-supervised), we study the existence of matching subnetworks that are transferable to many downstream tasks, with little performance degradation compared to using full pre-trained weights. We find <em>task-agnostic, universally transferable</em> subnetworks at pre-trained initialization, for <strong>classification</strong>, <strong>detection</strong>, and <strong>segmentation</strong> tasks.  <br><br> <strong>Abstract:</strong> The computer vision world has been re-gaining enthusiasm in various pre-trained models, including both classical ImageNet supervised pre-training and recently emerged self-supervised pre-training such as simCLR and MoCo. Pre-trained weights often boost a wide range of downstream tasks including classification, detection, and segmentation. Latest studies suggest that the pre-training benefits from gigantic model capacity. We are hereby curious and ask: after pre-training, does a pre-trained model indeed have to stay large for its universal downstream transferability? 
In this paper, we examine the supervised and self-supervised pre-trained models through the lens of <em>lottery ticket hypothesis</em> (LTH). LTH identifies highly sparse <em>matching subnetworks</em> that can be trained in isolation from (nearly) scratch, to reach the full models' performance. We extend the scope of LTH to questioning whether matching subnetworks still exist in the pre-training models, that enjoy the same downstream transfer performance. Our extensive experiments convey an overall positive message: from all pre-trained weights obtained by ImageNet classification, simCLR and MoCo, we are consistently able to locate such matching subnetworks at 59.04% to 96.48% sparsity that transfer <em>universally</em> to multiple downstream tasks, whose performance see no degradation compared to using full pre-trained weights. Further analyses reveal that subnetworks found from different pre-training tend to yield diverse mask structures and perturbation sensitivities. We conclude that the core LTH observations remain generally relevant in the pre-training paradigm of computer vision, but more delicate discussions are needed in some cases.</p></p>
        
                                <h2>Main Results of Universal Pre-training Tickets</h2>
        
                <p><p align="center">
    <img src="assets/cls.png" width="85%">
    <img src="assets/dense.png" width="85%">
    <img src="assets/CV_caption.png" width="85%">
</p></p>
        
                                <h2>Citation</h2>
        
        
                <pre class="highlight"> @article{chen2021lottery,
    title={The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models},
    author={Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Carbin, Michael and Wang, Zhangyang},
    journal={arXiv preprint arXiv:2012.06908},
    year={2021}
  }

@inproceedings{chen2020lottery,
    author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
    pages = {15834--15846},
    publisher = {Curran Associates, Inc.},
    title = {The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
    url = {https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf},
    volume = {33},
    year = {2020}
  }
</pre>
                        
                <p><p><strong>Acknowledgments</strong>: TBD.</p></p>
        
                
      </div>
    </div> <!-- row -->

  </div> <!-- container -->

  <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function () {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.maxHeight) {
          content.style.maxHeight = null;
        } else {
          content.style.maxHeight = content.scrollHeight * 50 + "px";
        }
        content.style.height = "550%";
      });
    }
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
  
  <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</body>

</html>