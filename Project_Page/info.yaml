title: 
    Universal Transferrable Tickets from Pre-training

authors:
    Tianlong Chen: 
        url: https://tianlong-chen.github.io/about/
        affiliation: 1
    Jonathan Frankle: 
        url: http://www.jfrankle.com
        affiliation: 2
    Shiyu Chang:
        url: https://people.csail.mit.edu/chang87/
        affiliation: 3
    Sijia Liu: 
        url: https://lsjxjtu.github.io/index.html
        affiliation: 3, 4
    Yang Zhang: 
        url: https://zhangyangbill.github.io/My-webpage/
        affiliation: 3
    Michael Carbin: 
        url: https://people.csail.mit.edu/mcarbin/
        affiliation: 2
    Zhangyang Wang: 
        url: https://vita-group.github.io
        affiliation: 1

affiliations:
    - University of Texas at Austin
    - MIT CSAIL
    - MIT-IBM Watson AI Lab, IBM Research
    - Michigan State University

beamers:
    NeurIPS 2020 Paper:
        img: assets/bert_paper.png
        url: https://arxiv.org/pdf/2007.12223.pdf
        width: 50
        height: 60
    NeurIPS 2020 Poster:
        img: assets/bert_poster.png
        url: https://docs.google.com/presentation/d/1ntlPnIqSfZAtys4hPwl6t8N63oA6_Ym92G24F42n3wg/edit?usp=sharing
        width: 100
        height: 60
    NeurIPS 2020 Code:
        img: assets/bert_code.png
        url: https://github.com/VITA-Group/BERT-Tickets
        width: 50
        height: 60
    CVPR 2021 Paper:
        img: assets/CV_paper.png
        url: https://arxiv.org/pdf/2012.06908.pdf
        width: 50
        height: 60
    CVPR 2021 Poster:
        img: assets/CV_poster.png
        url: https://docs.google.com/presentation/d/1lYUo1qpcYRbzm2qYh1mIKjcd8Kg42_oARX1o-44EL4I/edit?usp=sharing
        width: 100
        height: 60
    CVPR 2021 Code:
        img: assets/CV_code.png
        url: https://github.com/VITA-Group/CV_LTH_Pre-training
        width: 50
        height: 60

paragraphs:
    - title: (NeurIPS'20) The Lottery Tickets Hypothesis for Pre-trained BERT Networks
      content: >-
        <p align="center">
            <img src="assets/bert_transfer.png" width="80%">
        </p>
        **Abstract:** In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the *lottery ticket hypothesis* has shown that models for NLP and computer vision contain smaller *matching* subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40% to 90% sparsity. We find these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer *universally*; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context.
    
    #- rawhtml: >-
    #    <p align="center">Note: We provide
    #      <a href="https://colab.research.google.com/gist/Lyken17/91b81526a8245a028d4f85ccc9191884/deep-leakage-from-gradients.ipynb"
    #        target="_parent"><img
    #          src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667"
    #          alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a>
    #      for quick reproduction!
    #    </p>
    
    #- content: >-
    #    Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as **Deep Leakage from Gradients** and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is _pixel-wise_ accurate for images and _token-wise_ matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is [gradient pruning.](https://hanlab.mit.edu) 
    #    $$ \max(2 T_{\text{latency}} + T_{\text{sync}} - d \times T_{\text{computation}}, 0) / T_{\text{compute}} $$
    #    $$ \frac{1}{N} \sum_{n=0}^{N-1} \mathbb{E} [ \Vert \nabla f(\overline{w}_{n}) \Vert ^2 ]  = O \left (\frac{\Delta+\sigma^2}{\sqrt{JN}}+ \frac{Jd^2}{N} \right ). $$

    #- content: >-
    #    <p align="center">
    #        <img src="assets/nips-dlg.jpg" width="80%">
    #    </p>

    - title: (CVPR'21) The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models
      content: >-
        <p align="center">
            <img src="assets/T.gif" width="60%">
        </p>
        Overview of our work paradigm: from pre-trained CV models (both supervised and self-supervised), we study the existence of matching subnetworks that are transferable to many downstream tasks, with little performance degradation compared to using full pre-trained weights. We find *task-agnostic, universally transferable* subnetworks at pre-trained initialization, for **classification**, **detection**, and **segmentation** tasks. 
        <br><br>
        **Abstract:** The computer vision world has been re-gaining enthusiasm in various pre-trained models, including both classical ImageNet supervised pre-training and recently emerged self-supervised pre-training such as simCLR and MoCo. Pre-trained weights often boost a wide range of downstream tasks including classification, detection, and segmentation. Latest studies suggest that the pre-training benefits from gigantic model capacity. We are hereby curious and ask: after pre-training, does a pre-trained model indeed have to stay large for its universal downstream transferability? 
        
        In this paper, we examine the supervised and self-supervised pre-trained models through the lens of *lottery ticket hypothesis* (LTH). LTH identifies highly sparse *matching subnetworks* that can be trained in isolation from (nearly) scratch, to reach the full models' performance. We extend the scope of LTH to questioning whether matching subnetworks still exist in the pre-training models, that enjoy the same downstream transfer performance. Our extensive experiments convey an overall positive message: from all pre-trained weights obtained by ImageNet classification, simCLR and MoCo, we are consistently able to locate such matching subnetworks at 59.04% to 96.48% sparsity that transfer *universally* to multiple downstream tasks, whose performance see no degradation compared to using full pre-trained weights. Further analyses reveal that subnetworks found from different pre-training tend to yield diverse mask structures and perturbation sensitivities. We conclude that the core LTH observations remain generally relevant in the pre-training paradigm of computer vision, but more delicate discussions are needed in some cases.
      #rawhtml: >-
      #  <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">deep_leakage_from_gradients</span>(model, origin_grad): 
      #      dummy_data <span style="color: #333333">=</span> torch<span style="color: #333333">.</span>randn(origin_data<span style="color: #333333">.</span>size())
      #      dummy_label <span style="color: #333333">=</span>  torch<span style="color: #333333">.</span>randn(dummy_label<span style="color: #333333">.</span>size())
      #      optimizer <span style="color: #333333">=</span> torch<span style="color: #333333">.</span>optim<span style="color: #333333">.</span>LBFGS([dummy_data, dummy_label] )
      #    
      #      <span style="color: #008800; font-weight: bold">for</span> iters <span style="color: #000000; font-weight: bold">in</span> <span style="color: #007020">range</span>(<span style="color: #0000DD; font-weight: bold">300</span>):
      #        <span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">closure</span>():
      #          optimizer<span style="color: #333333">.</span>zero_grad()
      #          dummy_pred <span style="color: #333333">=</span> model(dummy_data) 
      #          dummy_loss <span style="color: #333333">=</span> criterion(dummy_pred, dummy_label) 
      #          dummy_grad <span style="color: #333333">=</span> grad(dummy_loss, model<span style="color: #333333">.</span>parameters(), create_graph<span style="color: #333333">=</span><span style="color: #007020">True</span>)
      #    
      #          grad_diff <span style="color: #333333">=</span> <span style="color: #007020">sum</span>(((dummy_grad <span style="color: #333333">-</span> origin_grad) <span style="color: #333333">**</span> <span style="color: #0000DD; font-weight: bold">2</span>)<span style="color: #333333">.</span>sum() \
      #            <span style="color: #008800; font-weight: bold">for</span> dummy_g, origin_g <span style="color: #000000; font-weight: bold">in</span> <span style="color: #007020">zip</span>(dummy_grad, origin_grad))
      #          
      #          grad_diff<span style="color: #333333">.</span>backward()
      #          <span style="color: #008800; font-weight: bold">return</span> grad_diff
      #        
      #        optimizer<span style="color: #333333">.</span>step(closure)
      #        
      #      <span style="color: #008800; font-weight: bold">return</span>  dummy_data, dummy_label
      #    </pre></div>
    
    - title: Main Results of Universal Pre-training Tickets
      content: >-
        <p align="center">
            <img src="assets/cls.png" width="85%">
            <img src="assets/dense.png" width="85%">
        </p>
        **Figure**: Performance of IMP subnetworks with a range of sparsity from $0.00\%$ to $98.20\%$ (i.e., remaining weight from $100\%$ to $1.80\%$) on downstream classification tasks, including CIFAR-10, CIFAR-100, SVHN and Fashion-MNIST. ($m_{\mathrm{Img}},\theta_{\mathrm{Img}}$), ($m_{\mathrm{sim}},\theta_{\mathrm{sim}}$) and ($m_{\mathrm{MoCo}},\theta_{\mathrm{MoCo}}$) denote transfer performance of subnetworks found at pre-training tasks. Subnetworks with ($m_{T_i},\theta_{p}$), $T_i\in$(CIFAR-10, CIFAR-100, SVHN, Fashion-MNIST) and $\theta_p\in(\theta_{\mathrm{Img}},\theta_{\mathrm{sim}},\theta_{\mathrm{MoCo}})$ are identified on the downstream task $T_i$ with pre-trained weights $\theta_p$. Subnetworks ($m_{T_i},\theta_{0}$) and ($m_{T_i},\theta_{5\%}$) are found on the task $T_i$ with the random initialization $\theta_0$ and an early rewinding weights $\theta_{5\%}$. Subnetworks with ($m_{\mathrm{VOC2007}},\theta_{p}$) and ($m_{\mathrm{VOC2012}},\theta_{p}$), $\theta_p\in(\theta_{\mathrm{Img}},\theta_{\mathrm{sim}},\theta_{\mathrm{MoCo}})$ are identified on the downstream detection and segmentation tasks with pre-trained weights $\theta_p$, respectively. Curves with errors (shadow regions) are the average across three independent runs, with the standard deviations: same hereinafter.
    
    # - title: Results on Batches Images
    #   content: >-
    #     <p align="center">
    #         <img src="assets/out.gif" width="66%">
    #     </p>
    
    #- title: Results on Language Model
    #  content: >-
    #    <p align="center">
    #        <img src="assets/nlp_results.png" width="66%">
    #    </p>
    
    #- title: Introduction Video
    #  content: >-
    #    <div class="container_2" align="center">
    #        <iframe src="https://www.youtube.com/embed/kMVvT8cf3Iw" frameborder="0" allowfullscreen class="video"></iframe>
    #    </div>

    - title: Citation
      rawhtml: >-
        <pre class="highlight">
        @article{chen2021lottery,
            title={The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models},
            author={Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Carbin, Michael and Wang, Zhangyang},
            journal={arXiv preprint arXiv:2012.06908},
            year={2021}
          }

        @inproceedings{chen2020lottery,
            author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
            booktitle = {Advances in Neural Information Processing Systems},
            editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
            pages = {15834--15846},
            publisher = {Curran Associates, Inc.},
            title = {The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
            url = {https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf},
            volume = {33},
            year = {2020}
          }
        </pre>
    
    - content: >-
        **Acknowledgments**: TBD.