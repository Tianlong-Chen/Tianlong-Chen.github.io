<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianlong-chen.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="I am currently a fourth-year Ph.D. candidate of Electrical and Computer Engineering (DICE) at VITA, The University of Texas at Austin, advised by Dr. Zhangyang (Atlas) Wang. My research interests in">
<meta property="og:type" content="website">
<meta property="og:title" content="Hello World!">
<meta property="og:url" content="https://tianlong-chen.github.io/about/index.html">
<meta property="og:site_name" content="Tianlong Chen (陈天龙)">
<meta property="og:description" content="I am currently a fourth-year Ph.D. candidate of Electrical and Computer Engineering (DICE) at VITA, The University of Texas at Austin, advised by Dr. Zhangyang (Atlas) Wang. My research interests in">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-09-30T22:22:01.751Z">
<meta property="article:modified_time" content="2022-09-30T22:22:01.745Z">
<meta property="article:author" content="Tianlong Chen (陈天龙)">
<meta property="article:tag" content="Ph.D Student">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianlong-chen.github.io/about/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hello World! | Tianlong Chen (陈天龙)
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tianlong Chen (陈天龙)</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">What does not kill you makes you stronger</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-publication">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Publication</a>

  </li>
        <li class="menu-item menu-item-bio">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>Bio</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-topics">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Topics</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Tianlong-Chen" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">Hello World!
</h1>

<div class="post-meta">
  

</div>

</header>

      
      
      
      <div class="post-body">
          <p> <img src = "../images/TLC_1.JPG" align = "left" width="24%" hight="10%"> I am currently a fourth-year Ph.D. candidate of Electrical and Computer Engineering (<a href="http://www.ece.utexas.edu/research/areas/commnets" target="_blank" rel="noopener">DICE</a>) at <a href="https://vita-group.github.io" target="_blank" rel="noopener">VITA</a>, The University of Texas at Austin, advised by <a href="https://vita-group.github.io" target="_blank" rel="noopener">Dr. Zhangyang (Atlas) Wang</a>. My research interests include Sparse Neural Network &amp; Lottery Ticket Hypothesis, AutoML &amp; Learning to Optimize, Adversarial Robustness, Graph Neural Networks, and Self-Supervision. I am a recipient of <a href="https://www.research.ibm.com/university/awards/fellowships-awardees.html" target="_blank" rel="noopener">2021 IBM Ph.D. Fellowship</a>, 2021 Graduate Dean’s Prestigious Fellowship, <a href="https://research.adobe.com/fellowship/previous-fellowship-award-winners/" target="_blank" rel="noopener">2022 Adobe Ph.D. Fellowship</a>. [<a href="https://drive.google.com/file/d/18-Q4Ml6X-Yk90ar1WnC13eE0Mg7jfc6q/view?usp=sharing" target="_blank" rel="noopener">Resume</a>] [<a href="https://scholar.google.com/citations?user=LE3ctn0AAAAJ" target="_blank" rel="noopener">Google Scholar</a>] [<a href="https://tianlong-chen.github.io/archives/">Publication</a>] </p>
<br>

<h3 id="Education"><a href="#Education" class="headerlink" title="Education"></a>Education</h3><ul>
<li>[Aug. 2020 - Present] <em>Ph.D.</em> candidate in Electrical and Computer Engineering, <a href="http://www.ece.utexas.edu/research/areas/commnets" target="_blank" rel="noopener">DICE</a>, The University of Texas at Austin</li>
<li>[Aug. 2018 - Aug. 2020] <em>Ph.D.</em> student in Computer Science, Texas A&amp;M University</li>
<li>[Aug. 2013 - Jun. 2017] <em>B.S.c.</em> in Applied Mathematics and <em>B.Eng.</em> (Dual) in Computer Science, <a href="https://en.scgy.ustc.edu.cn" target="_blank" rel="noopener">School of the Gifted Young</a>, University of Science and Technology of China</li>
</ul>
<h3 id="Selected-Conference-Paper"><a href="#Selected-Conference-Paper" class="headerlink" title="Selected Conference Paper"></a>Selected Conference Paper</h3><p>[*equal contribution]</p>
<h4 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h4><p><img src = "../images/NIPS22_MOE_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’22]</strong> <a href="">M3ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design</a><br>H. Liang, Z. Fan, R. Sarkar, Z. Jiang, <strong>T. Chen</strong>, K. Zou, Y. Cheng, C. Hao, and Z. Wang. [<a href="">Paper</a>] [<a href="https://github.com/VITA-Group/M3ViT" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/NIPS22_Backdoor_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’22]</strong> <a href="">Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets</a><br>R. Cai, Z. Zhang, <strong>T. Chen</strong>, X. Chen, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/NIPS22_init_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’22]</strong> <a href="">Old can be Gold: Better Gradient Flow can make Vanilla-GCNs Great Again</a><br>A. Jaiswal, P. Wang, <strong>T. Chen</strong>, J. Rousseau, Y. Ding, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/NIPS22_LTH_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’22]</strong> <a href="">Sparse Winning Tickets are Data-Efficient Image Recognizers</a><br>M. Varma<em>, X. Chen</em>, Z. Zhang, <strong>T. Chen</strong>, S. Venugopalan, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/NIPS22_HP_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’22]</strong> <a href="">Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative</a><br>T. Wei, Y. You, <strong>T. Chen</strong>, Y. Shen, J. He, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/NIPS22_BLP_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’22]</strong> <a href="">Advancing Model Pruning via Bi-level Optimization</a><br>Y. Zhang, Y. Yao, P. Ram, P. Zhao, <strong>T. Chen</strong>, M. Hong, Y. Wang, S. Liu. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/NIPS22_GCN_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’22 Dataset and Benchmark]</strong> <a href="">A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking</a><br>K. Duan, Z. Liu, P. Wang, W. Zheng, K. Zhou, <strong>T. Chen</strong>, X. Hu, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ECCV22_1.png" align = "left" width="12%" hight="12%"><strong>[ECCV’22]</strong> <a href="">Scalable Learning to Optimize: A Learned Optimizer Can Train Big Models</a><br>X. Chen*, <strong>T. Chen*</strong>, Y. Cheng, W. Chen, A. Awadallah, and Z. Wang. [<a href="">Paper</a>] [<a href="https://github.com/VITA-Group/Scalable-L2O" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ECCV22_3.png" align = "left" width="12%" hight="12%"><strong>[ECCV’22]</strong> <a href="">DnA: Improving Few-shot Transfer Learning with Low-Rank Decomposition and Alignment</a><br>Z. Jiang, <strong>T. Chen</strong>, X. Chen, Y. Cheng, L. Zhou, L. Yuan, A. Awadallah, and Z. Wang. [<a href="">Paper</a>] [<a href="https://github.com/VITA-Group/DnA" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ECCV22_2.png" align = "left" width="12%" hight="12%"><strong>[ECCV’22]</strong> <a href="">Point Cloud Domain Adaptation via Masked Local 3D Structure Prediction</a><br>H. Liang, H. Fan, Z. Fan, Y. Wang, <strong>T. Chen</strong>, Y. Cheng, and Z. Wang. [<a href="">Paper</a>] [<a href="https://github.com/VITA-Group/MLSP" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p> <img src = "../images/ICML_6_22_1.png" align = "left" width="12%" hight="12%"><strong>[ICML’22]</strong> <a href="">Data-Efficient Double-Win Lottery Tickets from Robust Pre-training</a><br><strong>T. Chen</strong>, Z. Zhang, S. Liu, Y. Zhang, S. Chang, and Z. Wang. [<a href="https://arxiv.org/pdf/2206.04762.pdf">Paper</a>] [<a href="https://github.com/VITA-Group/Double-Win-LTH" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICML_5_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’22]</strong> <a href="">Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets</a><br><strong>T. Chen</strong>, X. Chen, X. Ma, Y. Wang, and Z. Wang. [<a href="https://arxiv.org/pdf/2202.04736.pdf">Paper</a>] [<a href="https://github.com/VITA-Group/Structure-LTH" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICML_4_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’22]</strong> <a href="">Linearity Grafting: How Neuron Pruning Helps Certifiable Robustness</a><br><strong>T. Chen*</strong>, H. Zhang*, Z. Zhang, S. Chang, S. Liu, P. Chen, and Z. Wang. [<a href="https://arxiv.org/pdf/2206.07839.pdf">Paper</a>] [<a href="https://github.com/VITA-Group/Linearity-Grafting" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICML_3_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML22]</strong> <a href="">Universality of Winning Tickets: A Renormalization Group Perspective</a><br>W. Redman, <strong>T. Chen</strong>, Z. Wang, and A. Dogra. [<a href="https://arxiv.org/pdf/2110.03210.pdf">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICML_2_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’22]</strong> <a href="">Training Your Sparse Neural Network Better with Any Mask</a><br>A. Jaiswal, H. Ma, <strong>T. Chen</strong>, Y. Ding, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICML_1_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’22]</strong> <a href="">Neural Implicit Dictionary Learning via Mixture-of-Expert Training</a><br>P. Wang, Z. Fan, <strong>T. Chen</strong>, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/CVPR_4_22_1.png" align = "left" width="12%" hight="12%"> <strong>[CVPR22]</strong> <a href="">The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy</a><br><strong>T. Chen</strong>, Z. Zhang, Y. Chang, A. Awadallah, and Z. Wang. [<a href="https://arxiv.org/pdf/2203.06345.pdf">Paper</a>] [<a href="https://github.com/VITA-Group/Diverse-ViT" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/CVPR_3_22_1.png" align = "left" width="12%" hight="12%"> <strong>[CVPR’22]</strong> <a href="">Aug-NeRF: Training Stronger Neural Radiance Fields with Triple-Level Augmentations</a><br><strong>T. Chen*</strong>, P. Wang*, Z. Fan, and Z. Wang. [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Aug-NeRF_Training_Stronger_Neural_Radiance_Fields_With_Triple-Level_Physically-Grounded_Augmentations_CVPR_2022_paper.pdf">Paper</a>] [<a href="https://github.com/VITA-Group/Aug-NeRF" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/CVPR_2_22_1.png" align = "left" width="12%" hight="12%"> <strong>[CVPR’22]</strong> <a href="">Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free</a><br><strong>T. Chen*</strong>, Z. Zhang*, Y. Zhang*, S. Chang, S. Liu, and Z. Wang. [<a href="https://arxiv.org/pdf/2205.11819.pdf">Paper</a>] [<a href="https://github.com/VITA-Group/Backdoor-LTH" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/CVPR_1_22_1.png" align = "left" width="12%" hight="12%"> <strong>[CVPR’22 Oral]</strong> <a href="">CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings</a><br>Z. Fan, <strong>T. Chen</strong>, P. Wang, and Z. Wang. [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.pdf">Paper</a>] [<a href="https://github.com/VITA-Group/CADTransformer" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_10_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR‘22]</strong> <a href="">Sparsity Winning Twice: Better Robust Generalization from More Efficient Training</a><br><strong>T. Chen*</strong>, Z. Zhang*, P. Wang*, S. Balachandra*, H. Ma*, Z. Wang and Z. Wang. [<a href="https://openreview.net/pdf?id=SYuJXrXq8tw">Paper</a>] [<a href="https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_9_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’22]</strong> <a href="">Unified Visual Transformer Compression</a><br>S. Xing*, <strong>T. Chen*</strong>, J. Shen, H. Yuan, J. Tan, S. Yang, J. Liu and Z. Wang. [<a href="https://openreview.net/pdf?id=9jsZiUgkCZP">Paper</a>] [<a href="https://github.com/VITA-Group/UVC" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_8_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’22]</strong> <a href="">Audio Lottery: Speech Recognition Made Ultra-Li- ghtweight, Noise-Robust, and Transferable</a><br>S. Ding*, <strong>T.Chen*</strong> and Z. Wang. [<a href="https://openreview.net/pdf?id=9Nk6AJkVYB">Paper</a>] [<a href="https://github.com/VITA-Group/Audio-Lottery" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_7_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’22 Spotlight]</strong> <a href="">Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining</a><br>L. Miao, X. Luo, <strong>T. Chen</strong>, W. Chen, D. Liu and Z. Wang. [<a href="https://openreview.net/pdf?id=O1DEtITim__">Paper</a>] [<a href="https://github.com/VITA-Group/SFW-Once-for-All-Pruning" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_6_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’22]</strong> <a href="">The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training</a><br>S. Liu, <strong>T. Chen</strong>, X. Chen, L. Shen, D. Mocanu, Z. Wang and M. Pechenizkiy. [<a href="https://openreview.net/pdf?id=VBZJ_3tz-t">Paper</a>] [<a href="https://github.com/VITA-Group/Random_Pruning" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_5_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’22]</strong> <a href="">Deep Ensembling with No Overhead for either Training or Testing: The All-Round B- lessings of Dynamic Sparsity</a><br>S. Liu, <strong>T. Chen</strong>, Z. Atashgahi, X. Chen, G. Sokar, E. Mocanu, M. Pechenizkiy, Z. Wang and D. Mocanu. [<a href="https://openreview.net/pdf?id=RLtqs6pzj1-">Paper</a>] [<a href="https://github.com/VITA-Group/FreeTickets" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_4_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’22]</strong> <a href="">Scaling the Depth of Vision Transformers via the Fourier Domain Analysis</a><br>P. Wang, W. Zheng, <strong>T. Chen</strong> and Z. Wang. [<a href="https://openreview.net/pdf?id=O476oWmiNNp">Paper</a>] [<a href="https://github.com/VITA-Group/ViT-Anti-Oversmoothing" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_3_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’22]</strong> <a href="">Symbolic Learning to Optimize: Towards Interpretability and Scalability</a><br>W. Zheng, <strong>T. Chen</strong>, T. Hu and Z. Wang. [<a href="https://openreview.net/pdf?id=ef0nInZHKIC">Paper</a>] [<a href="https://github.com/VITA-Group/Symbolic-Learning-To-Optimize" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_2_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’22]</strong> <a href="">Optimizer Amalgamation</a><br>T. Huang, <strong>T. Chen</strong>, S. Liu, S. Chang, L. Amini and Z. Wang. [<a href="https://openreview.net/pdf?id=VqzXzA9hjaX">Paper</a>] [<a href="https://github.com/VITA-Group/OptimizerAmalgamation" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR_1_22_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’22]</strong> <a href="">Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How</a><br>Y. You, Y. Cao, <strong>T. Chen</strong>, Z. Wang and Y. Shen. [<a href="https://openreview.net/pdf?id=EVVadRFRgL7">Paper</a>] [<a href="https://github.com/Shen-Lab/Bayesian-L2O" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/automl_22_1.png" align = "left" width="12%" hight="12%"> <strong>[AutoML’22]</strong> <a href="">AutoCoG: A Unified Data-Model Co-Search Framework for Graph Neural Networks</a><br>D. Hoang, K. Zhou, <strong>T. Chen</strong>, X. Hu, and Z. Wang. [<a href="https://openreview.net/pdf?id=r0zIWWar8gq">Paper</a>] [<a href="https://github.com/VITA-Group/AutoCoG" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<p> <img src = "../images/AAAI_22_1.png" align = "left" width="12%" hight="12%"> <strong>[AAAI’22]</strong> <a href="https://tianlong-chen.github.io/AAAI-22/">Playing Lottery Tickets with Vision and Language</a><br>Z. Gan, Y. Chen, L. Li, <strong>T. Chen</strong>, Y. Cheng, S. Wang and J. Liu. [<a href="https://arxiv.org/pdf/2104.11832.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/AAAI-22/">Abstract</a>]</p>
<p> <img src = "../images/WSDM_22_1.png" align = "left" width="12%" hight="12%"> <strong>[WSDM’22]</strong> <a href="https://tianlong-chen.github.io/WSDM22/">Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations</a> Y. You, <strong>T. Chen</strong>, Z. Wang and Y. Shen. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/WSDM22/">Abstract</a>]</p>
<h4 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h4><p> <img src = "../images/NIPS21_ViT_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’21]</strong> <a href="https://tianlong-chen.github.io/NIPS21-ViT/">Chasing Sparsity in Vision Transformers: An End-to-End Exploration</a><br><strong>T. Chen</strong>, Y. Cheng, Z. Gan, L. Yuan, L. Zhang and Z. Wang. [<a href="https://arxiv.org/pdf/2106.04533.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/SViTE" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS21-ViT/">Abstract</a>]</p>
<p> <img src = "../images/Preprint_DE_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’21]</strong> <a href="https://tianlong-chen.github.io/NIPS21-DE/">Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective</a> <strong>T. Chen</strong>, Y. Cheng, Z. Gan, J. Liu, and Z. Wang. [<a href="https://arxiv.org/pdf/2103.00397.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS21-DE/">Abstract</a>]</p>
<p><img src = "../images/NIPS21_PLTH_1.png" align = "left" width="12%" hight="12%"> <strong>[NeurIPS’21]</strong> <a href="https://tianlong-chen.github.io/NIPS21-PLTH/">You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership</a> X. Chen*, <strong>T. Chen*</strong>, Z. Zhang and Z. Wang. [<a href="https://proceedings.neurips.cc/paper/2021/file/0dfd8a39e2a5dd536c185e19a804a73b-Paper.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/NO-stealing-LTH" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS21-PLTH/">Abstract</a>]</p>
<p> <img src = "../images/NIPS21_GN_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’21]</strong> <a href="https://tianlong-chen.github.io/NIPS21-GN/">Sparse Training via Boosting Pruning Plasticity with Neuroregeneration</a><br>S. Liu, <strong>T. Chen</strong>, X. Chen, Z. Atashgahi, L. Yin, H. Kou, L. Shen, M. Pechenizkiy, Z. Wang, and D. Mocanu. [<a href="https://arxiv.org/pdf/2106.10404.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/Shiweiliuiiiiiii/GraNet" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS21-GN/">Abstract</a>]</p>
<p> <img src = "../images/NIPS21_CL_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’21]</strong> <a href="https://tianlong-chen.github.io/NIPS21-CL/">Improving Contrastive Learning on Imbalanced Data via Open-World Sampling</a><br>Z. Jiang, <strong>T. Chen</strong>, T. Chen and Z. Wang. [<a href="https://papers.nips.cc/paper/2021/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/MAK" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS21-CL/">Abstract</a>]</p>
<p> <img src = "../images/NIPS21_SLTH_1.png" align = "left" width="12%" hight="12%"><strong>[NeurIPS’21]</strong> <a href="https://tianlong-chen.github.io/NIPS21-SLTH/">Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?</a> X. Ma, G. Yuan, X. Shen, <strong>T. Chen</strong>, X. Chen, X. Chen, N. Liu, M. Qin, S. Liu, Z. Wang and Y. Wang. [<a href="https://arxiv.org/pdf/2107.00166.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/boone891214/sanity-check-LTH" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS21-SLTH/">Abstract</a>]</p>
<p><img src = "../images/Preprint_GCN_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’21]</strong> <a href="https://tianlong-chen.github.io/ICML21-GCNLTH/">A Unified Lottery Ticket Hypothesis for Graph Neural Networks</a><br><strong>T. Chen*</strong>, Y. Sui*, X. Chen, A. Zhang, and Z. Wang. [<a href="https://arxiv.org/pdf/2102.06790.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/Unified-LTH-GNN" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICML21-GCNLTH/">Abstract</a>]</p>
<p><img src = "../images/ICML21_PAC_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’21]</strong> <a href="https://tianlong-chen.github.io/ICML21-PAC/">Efficient Lottery Ticket Finding: Less Data is More</a><br>Z. Zhang*, X. Chen*, <strong>T. Chen*</strong>, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/ICML21-PAC/">Abstract</a>]</p>
<p><img src = "../images/ICML21_JOAO_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’21 Long Talk]</strong> <a href="https://tianlong-chen.github.io/ICML21-JOAO/">Graph Contrastive Learning Automated</a><br>Y. You, <strong>T. Chen</strong>, Y. Shen, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/ICML21-JOAO/">Abstract</a>]</p>
<p><img src = "../images/ICML21_HA_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’21 Long Talk]</strong> <a href="https://tianlong-chen.github.io/ICML21-HA/">Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm</a><br>M. Zhu, <strong>T. Chen</strong>, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/ICML21-HA/">Abstract</a>]</p>
<p><img src = "../images/ICML21_SDCL_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’21]</strong> <a href="https://tianlong-chen.github.io/ICML21-SDCL/">Self-Damaging Contrastive Learning</a><br>Z. Jiang, <strong>T. Chen</strong>, B. Mortazavi, and Z. Wang. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/ICML21-SDCL/">Abstract</a>]</p>
<p><img src = "../images/Preprint_LTH_1.png" align = "left" width="12%" hight="12%"> <strong>[CVPR’21]</strong> <a href="https://tianlong-chen.github.io/CVPR21-LTH/">The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision</a><br><strong>T. Chen</strong>, J. Frankle, S. Chang, S. Liu, Y. Zhang, Z. Wang, and M. Carbin. [<a href="https://arxiv.org/pdf/2012.06908.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/CV_LTH_Pre-training" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/CVPR21-LTH/">Abstract</a>] [<a href="https://tianlong-chen.github.io/Project_Page/index.html">Project</a>]</p>
<p><img src = "../images/CVPR21_IQA_1.png" align = "left" width="12%" hight="12%"> <strong>[CVPR’21]</strong> <a href="">Troubleshooting Blind Image Quality Models in the Wild</a><br>Z. Wang, H. Wang, <strong>T. Chen</strong>, Z. Wang, and K. Ma. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="">Abstract</a>]</p>
<p><img src = "../images/ICLR21_Adv_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’21]</strong> <a href="https://tianlong-chen.github.io/ICLR21-ADV/">Robust Overfitting may be mitigated by properly learned smoothening</a><br><strong>T. Chen*</strong>, Z. Zhang*, S. Liu, S. Chang, and Z. Wang. [<a href="https://openreview.net/pdf?id=qZzy5urZw9" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/Alleviate-Robust-Overfitting" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICLR21-ADV/">Abstract</a>]</p>
<p><img src = "../images/ICLR21_LifeLT_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’21]</strong> <a href="https://tianlong-chen.github.io/ICLR21-LifeLT/">Long Live the Lottery: The Existence of Winning Tickets in Lifelong Learning</a><br><strong>T. Chen*</strong>, Z. Zhang*, S. Liu, S. Chang, and Z. Wang. [<a href="https://openreview.net/pdf?id=LXMSvPmsm0g" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/Lifelong-Learning-LTH" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICLR21-LifeLT/">Abstract</a>]</p>
<p><img src = "../images/ICLR21_GAN_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’21]</strong> <a href="https://tianlong-chen.github.io/ICLR21_GAN/">GANs Can Play Lottery Tickets Too</a><br>X. Chen, Z. Zhang, Y. Sui, and <strong>T. Chen</strong>. [<a href="https://openreview.net/pdf?id=1AoMhc_9jER" target="_blank" rel="noopener">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/ICLR21_GAN/">Abstract</a>]</p>
<p><img src = "../images/ICLR21_BadT_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’21 Spotlight Oral]</strong> <a href="https://tianlong-chen.github.io/ICLR21-NastyT/">Undistillable: Making A Nasty Teacher That CANNOT teach students</a><br>H. Ma, <strong>T. Chen</strong>, T. Hu, C. You, X. Xie, and Z. Wang. [<a href="https://openreview.net/pdf?id=0zvfm-nZqQs" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/Nasty-Teacher" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICLR21-NastyT/">Abstract</a>]</p>
<p><img src = "../images/ICLR21_L2O_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’21]</strong> <a href="https://tianlong-chen.github.io/ICLR21_L2O/">Learning A Minimax Optimizer: A Pilot Study</a><br>J. Shen, X. Chen, H. Heaton, <strong>T. Chen</strong>, J. Liu, W. Yin, and Z. Wang. [<a href="https://openreview.net/pdf?id=nkIDwI6oO4_" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/L2O-Minimax" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICLR21_L2O/">Abstract</a>]</p>
<p><img src = "../images/CVPRW21_1.png" align = "left" width="12%" hight="12%"><strong>[CVPRW’21 Spotlight Oral]</strong> <a href="https://tianlong-chen.github.io/CVPRW21/">BNN-BN=? Training Binary Neural Networks without Batch Normalization</a> </p>
<p><strong>T. Chen</strong>, Z. Zhang, X. Ouyang, Z. Liu, Z. Shen and Z. Wang. [<a href="https://arxiv.org/pdf/2104.08215.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/BNN_NoBN" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/CVPRW21/">Abstract</a>]</p>
<p><img src = "../images/ICASSP_21_1.png" align = "left" width="12%" hight="12%"> <strong>[ICASSP’21]</strong> <a href="https://tianlong-chen.github.io/ICASSP21/">VGAI: End-to-End Learning of Vision-Based Decentralized Controllers for Robust Swarms</a><br>T. Hu, F. Gama, <strong>T. Chen</strong>, Z. Wang, A. Ribeiro, and B. Sadler. [<a href="https://arxiv.org/pdf/2002.02308.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/ICASSP21/">Abstract</a>]</p>
<p><img src = "../images/AAAIW_21_1.png" align = "left" width="12%" hight="12%"> <strong>[AAAIW’21 Oral]</strong> <a href="https://tianlong-chen.github.io/AAAIW-21/">AR-Stock: Deep Augmented Relational Stock Prediction</a><br>T. Wei, Y. You, and <strong>T. Chen</strong>. [<a href="">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/AAAIW-21/">Abstract</a>]</p>
<h4 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h4><p><img src = "../images/NIPS20_L2O_1.png" align = "left" width="12%" hight="12%"> <strong>[NeurIPS’20 Spotlight Oral]</strong> <a href="https://tianlong-chen.github.io/NIPS20-L2O/">Training Stronger Baselines for Learning to Optimize</a><br><strong>T. Chen*</strong>, W. Zhang*, J. Zhou, S. Chang, S. Liu, L. Amini, and Z. Wang. [<a href="https://papers.nips.cc/paper/2020/file/51f4efbfb3e18f4ea053c4d3d282c4e2-Paper.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/L2O-Training-Techniques" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS20-L2O/">Abstract</a>]</p>
<p><img src = "../images/NIPS20_BERT_1.png" align = "left" width="12%" hight="12%"> <strong>[NeurIPS’20]</strong> <a href="https://tianlong-chen.github.io/NIPS20-BERT/">The Lottery Ticket Hypothesis for Pre-trained BERT Networks</a><br><strong>T. Chen</strong>, J. Frankle, S. Chang, S. Liu, Y. Zhang, Z. Wang, and M. Carbin. [<a href="https://arxiv.org/pdf/2007.12223.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/BERT-Tickets" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS20-BERT/">Abstract</a>] [<a href="https://tianlong-chen.github.io/Project_Page/index.html">Project</a>]</p>
<p><img src = "../images/NIPS20_CAT_1.png" align = "left" width="12%" hight="12%"> <strong>[NeurIPS’20]</strong> <a href="https://tianlong-chen.github.io/NIPS20-CAT/">Once-for-All Adversarial Training: In-Situ Trade off between Robustness and Accuracy for Free</a><br>H. Wang*, <strong>T. Chen*</strong>, S. Gui, T. Hu, J. Liu, and Z. Wang. [<a href="https://papers.nips.cc/paper/2020/file/537d9b6c927223c796cac288cced29df-Paper.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/Once-for-All-Adversarial-Training" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS20-CAT/">Abstract</a>]</p>
<p><img src = "../images/NIPS20_GCNCL_1.png" align = "left" width="12%" hight="12%"> <strong>[NeurIPS’20]</strong> <a href="https://tianlong-chen.github.io/NIPS20-GCNCL/">Graph Contrastive Learning with Augmentations</a><br>Y. You*, <strong>T. Chen*</strong>, Y. Sui, T. Chen, Z. Wang, and S. Yang. [<a href="https://papers.nips.cc/paper/2020/file/3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS20-GCNCL/">Abstract</a>]</p>
<p><img src = "../images/NIPS20_ADVCL_1.png" align = "left" width="12%" hight="12%"> <strong>[NeurIPS’20]</strong> <a href="https://tianlong-chen.github.io/NIPS20-ADVCL/">Robust Pre-Training by Adversarial Contrastive Learning</a><br>Z. Jiang, <strong>T. Chen</strong>, T. Chen, and Z. Wang. [<a href="https://arxiv.org/pdf/2010.13337.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/ACL_Neurips20" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS20-ADVCL/">Abstract</a>]</p>
<p><img src = "../images/InterSpeech20_1.png" align = "left" width="12%" hight="12%"> <strong>[InterSpeech’20]</strong> <a href="https://tianlong-chen.github.io/InterSpeech20/">AutoSpeech: Neural Architecture Search for Speaker Recognition</a><br>S. Ding*, <strong>T. Chen*</strong>, X. Gong, W. Zha, and Z. Wang. [<a href="https://arxiv.org/pdf/2005.03215.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/TAMU-VITA/AutoSpeech" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/InterSpeech20/">Abstract</a>]</p>
<p><img src = "../images/ECCV20_HALO_1.png" align = "left" width="12%" hight="12%"> <strong>[ECCV’20]</strong> <a href="https://tianlong-chen.github.io/ECCV20-HALO/">HALO: Hardware-Aware Learning to Optimize</a><br> C. Li*, <strong>T. Chen*</strong>, H. You, Z. Wang, and Y. Lin. [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540477.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/RICE-EIC/HALO" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ECCV20-HALO/">Abstract</a>]</p>
<p> <img src = "../images/ICML20_GCN_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’20]</strong> <a href="https://tianlong-chen.github.io/ICML20-GCN/">When Does Self-Supervision Help Graph Convolutional Networks?</a><br>Y. You*, <strong>T. Chen*</strong>, Z. Wang, and Y. Shen. [<a href="https://arxiv.org/pdf/2006.09136.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/Shen-Lab/SS-GCNs" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICML20-GCN/">Abstract</a>]</p>
<p> <img src = "../images/ICML20_Self_1.png" align = "left" width="12%" hight="12%"> <strong>[ICML’20]</strong> <a href="https://tianlong-chen.github.io/ICML20-Self/">Self-PU: Self Boosted and Calibrated Positive-Unlabeled Training</a><br>X. Chen, W. Chen, <strong>T. Chen</strong>, Y. Yuan, C. Gong, K. Chen, and Z. Wang. [<a href="https://arxiv.org/pdf/2006.11280.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/TAMU-VITA/Self-PU" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICML20-Self/">Abstract</a>]</p>
<p> <img src = "../images/CVPR20_Selfie_1.png" align = "left" width="12%" hight="12%"> <strong>[CVPR’20]</strong> <a href="https://tianlong-chen.github.io/CVPR20-Selfie/">Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning</a><br><strong>T. Chen</strong>, S. Liu, S. Chang, Y. Cheng, L. Amini, and Z. Wang. [<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Adversarial_Robustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/TAMU-VITA/Adv-SS-Pretraining" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/CVPR20-Selfie/">Abstract</a>]</p>
<p> <img src = "../images/CVPR20_GCN_1.png" align = "left" width="12%" hight="12%"> <strong>[CVPR’20]</strong> <a href="https://tianlong-chen.github.io/CVPR20-GCN/">L^2-GCN: Layer-Wise and Learned Efficient Training of Graph Convolutional Networks</a><br>Y. You*, <strong>T. Chen*</strong>, Z. Wang, and Y. Shen. [<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/You_L2-GCN_Layer-Wise_and_Learned_Efficient_Training_of_Graph_Convolutional_Networks_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/Shen-Lab/L2-GCN" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/CVPR20-GCN/">Abstract</a>]</p>
<p> <img src = "../images/ICLR20_Triple_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’20]</strong> <a href="https://tianlong-chen.github.io/ICLR20-Triple/">Triple Wins: Boosting Accuracy, Robustness and Efficiency by Enabling Input-Adaptive Inference</a><br>T. Hu*, <strong>T. Chen*</strong>, H. Wang, and Z. Wang. [<a href="https://openreview.net/pdf?id=rJgzzJHtDB" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/TAMU-VITA/triple-wins" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICLR20-Triple/">Abstract</a>]</p>
<p> <img src = "../images/ICLR20_MAD_1.png" align = "left" width="12%" hight="12%"> <strong>[ICLR’20]</strong> <a href="https://tianlong-chen.github.io/ICLR20-MAD/">I am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively </a><br>H. Wang, <strong>T. Chen</strong>, Z. Wang, and K. Ma. [<a href="https://openreview.net/pdf?id=rJehNT4YPr" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/TAMU-VITA/MAD" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICLR20-MAD/">Abstract</a>]</p>
<h4 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h4><p> <img src = "../images/NIPS19_1.png" align = "left" width="12%" hight="12%"> <strong>[NeurIPS’19]</strong> <a href="https://tianlong-chen.github.io/NIPS19/">Learning to Optimize in Swarms</a><br>Y. Cao, <strong>T. Chen</strong>, Z. Wang, and S. Yang. [<a href="https://papers.nips.cc/paper/9641-learning-to-optimize-in-swarms.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/Shen-Lab/LOIS" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/NIPS19/">Abstract</a>]</p>
<p> <img src = "../images/ICCV19_1.png" align = "left" width="12%" hight="12%"> <strong>[ICCV’19]</strong> <a href="https://tianlong-chen.github.io/ICCV19-ABD/">ABD-Net: Attentive but Diverse Person Re-Identification</a><br><strong>T. Chen</strong>, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and Z. Wang. [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_ABD-Net_Attentive_but_Diverse_Person_Re-Identification_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/TAMU-VITA/ABD-Net" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/ICCV19-ABD/">Abstract</a>]</p>
<h3 id="Selected-Journal-Paper"><a href="#Selected-Journal-Paper" class="headerlink" title="Selected Journal Paper"></a>Selected Journal Paper</h3><p> <img src = "../images/Preprint_L2O_1.png" align = "left" width="12%" hight="12%"><strong>[JMLR’22]</strong> <a href="https://tianlong-chen.github.io/Preprint-L2O/">Learning to Optimize: A Primer and A Benchmark</a><br>(α-β) <strong>T. Chen</strong>, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang, and W.Yin. [<a href="https://arxiv.org/pdf/2103.12828v1.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/VITA-Group/Open-L2O" target="_blank" rel="noopener">Code</a>] [<a href="https://tianlong-chen.github.io/Preprint-L2O/">Abstract</a>]</p>
<p><img src = "../images/tpami_22_1.png" align = "left" width="12%" hight="12%"> <strong>[TPAMI’22]</strong> <a href="">Bag of Tricks for Training Deeper Graph Neural Networks: A Comprehensive Benchmark Study</a><br><strong>T.Chen*</strong>, K. Zhou*, K. Duan, W. Zheng, P. Wang, X. Hu, and Z. Wang. [<a href="https://arxiv.org/pdf/2108.10521.pdf">Paper</a>] [<a href="https://github.com/VITA-Group/Deep_GCN_Benchmarking" target="_blank" rel="noopener">Code</a>] [<a href="">Abstract</a>]</p>
<h3 id="Experience"><a href="#Experience" class="headerlink" title="Experience"></a>Experience</h3><ul>
<li><p>Google, Research Intern [May 2022 - Aug. 2022]</p>
<ul>
<li>Mentor: <a href="https://scholar.google.com/citations?user=l1hP40AAAAAJ&hl=en" target="_blank" rel="noopener">Dr. Xianzhi Du</a>, <a href="https://dennyzhou.github.io/" target="_blank" rel="noopener">Dr. Denny Zhou</a></li>
</ul>
</li>
<li><p>IBM T. J. Watson Research Center, Part-time Research Intern [Oct. 2021 - Dec. 2021]</p>
<ul>
<li>Mentor: <a href="https://www.linkedin.com/in/horsts/" target="_blank" rel="noopener">Dr. Horst Samulowitz</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-lisa.amini" target="_blank" rel="noopener">Dr. Lisa Amini</a></li>
</ul>
</li>
<li><p>Facebook Research, Research Intern [May. 2021 - Aug. 2021]</p>
<ul>
<li>Mentor: <a href="http://students.cse.tamu.edu/xingwang/" target="_blank" rel="noopener">Dr. Xing Wang</a>, <a href="https://www.linkedin.com/in/chenatuc/" target="_blank" rel="noopener">Dr. Jason Chen</a></li>
</ul>
</li>
<li><p>Microsoft Research Redmond, Part-time Research Intern [Sep. 2020 - Dec. 2020]</p>
<ul>
<li>Mentor: <a href="https://sites.google.com/site/chengyu05/" target="_blank" rel="noopener">Dr. Yu Cheng</a>, <a href="https://zhegan27.github.io" target="_blank" rel="noopener">Dr. Zhe Gan</a></li>
</ul>
</li>
<li><p>Microsoft Research Redmond, Research Intern [May. 2020 - Aug. 2020]</p>
<ul>
<li>Mentor: <a href="https://sites.google.com/site/chengyu05/" target="_blank" rel="noopener">Dr. Yu Cheng</a>, <a href="https://zhegan27.github.io" target="_blank" rel="noopener">Dr. Zhe Gan</a>, <a href="https://www.linkedin.com/in/nkhuyu" target="_blank" rel="noopener">Dr. Yu Hu</a></li>
</ul>
</li>
<li><p>Walmart Technology, Research Intern [May. 2019 - Aug. 2019]</p>
<ul>
<li>Mentor: <a href="http://www.es.ele.tue.nl/~yyang/" target="_blank" rel="noopener">Dr. Yang Yang</a></li>
</ul>
</li>
<li><p>University of Science and Technology of China, Research Assistant [Jun. 2017 - Jun. 2018]</p>
<ul>
<li>Mentor: <a href="http://staff.ustc.edu.cn/~yangzw/" target="_blank" rel="noopener">Dr. Zhouwang Yang</a></li>
</ul>
</li>
<li><p>Harvard University, Research Intern [Jul. 2016 - Jan. 2017]</p>
<ul>
<li>Mentor: <a href="https://scholar.harvard.edu/gil" target="_blank" rel="noopener">Dr. Gil Alterovitz</a></li>
</ul>
</li>
</ul>
<h3 id="Research-Award"><a href="#Research-Award" class="headerlink" title="Research Award"></a>Research Award</h3><ul>
<li><a href="https://research.adobe.com/fellowship/previous-fellowship-award-winners/" target="_blank" rel="noopener">2022 Adobe Ph.D. Fellowship Award</a>, Jul. 2022</li>
<li>ICML 2022 Participation Grant, Jul. 2022</li>
<li>2021 Graduate Dean’s Prestigious Fellowship, Aug. 2021</li>
<li><a href="https://www.research.ibm.com/university/awards/fellowships-awardees.html" target="_blank" rel="noopener">2021 IBM Ph.D. Fellowship Award</a>, Aug. 2021</li>
<li><strong>3rd Place</strong>, <a href="http://wider-challenge.org/2019.html" target="_blank" rel="noopener">ICCV 2019 WIDER Challenge Track 4</a>, Oct. 2019</li>
<li><strong>First Place</strong>, Walmart-TAMU Person Re-Identification (Re-ID) Competition, May. 2018</li>
<li>NeurIPS 2019 Student Travel Award, Dec. 2019</li>
</ul>
<h3 id="Media-Coverage"><a href="#Media-Coverage" class="headerlink" title="Media Coverage"></a>Media Coverage</h3><ul>
<li><a href="https://news.mit.edu/2020/neural-model-language-1201" target="_blank" rel="noopener">Shrinking massive neural networks used to model language</a>, MIT News (Featured), Dec. 2020</li>
<li><a href="https://techxplore.com/news/2020-07-deep-neural-networks-adversarial-d.html" target="_blank" rel="noopener">Fooling deep neural networks for object detection with adversarial 3-D logos</a>, Tech Xplore, Jul. 2020</li>
</ul>
<h3 id="More-About-Me"><a href="#More-About-Me" class="headerlink" title="More About Me"></a>More About Me</h3><ul>
<li>I am a big fan of Pokemon. Playing Pokemon Go is one of my daily activities.</li>
<li>I also enjoy Hip-Hop and Country music. <a href="https://www.youtube.com/watch?v=p49e8vgHvh8" target="_blank" rel="noopener">Air (艾热)</a> is one of my favorite Chinese Hip-Hop stars.</li>
</ul>

      </div>
      
      
      
    </div>
    

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Tianlong Chen (陈天龙)"
      src="https://raw.githubusercontent.com/Tianlong-Chen/Shareable-Photos/master/Aboutme/TLC.JPG">
  <p class="site-author-name" itemprop="name">Tianlong Chen (陈天龙)</p>
  <div class="site-description" itemprop="description">Make the change that you want to see in the world</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Tianlong-Chen" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Tianlong-Chen" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tianlong.chen@utexas.edu" title="E-Mail → mailto:tianlong.chen@utexas.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?user=LE3ctn0AAAAJ" title="Google → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user&#x3D;LE3ctn0AAAAJ" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Google</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://express.adobe.com/page/CAdrFMJ9QeI2y/" title="https:&#x2F;&#x2F;express.adobe.com&#x2F;page&#x2F;CAdrFMJ9QeI2y&#x2F;" rel="noopener" target="_blank">My Ph.D. Advisor</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://github.com/VITA-Group" title="https:&#x2F;&#x2F;github.com&#x2F;VITA-Group" rel="noopener" target="_blank">VITA Github</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="snowflake-o"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tianlong Chen (陈天龙)</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
